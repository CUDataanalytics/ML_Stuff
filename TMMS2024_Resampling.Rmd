---
title: "Resampling Techniques for Handling Malaria Imbalance Data"
author: "D.K.MURIITHI"
date: "2024-07-10"
output:
  html_document:  
  word_document: default
  pdf_document: default
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 8, fig.width = 4,	message = FALSE, warning = FALSE,	comment = NA)
```
## confirmation and setting of working directory

```{r}
setwd("C:\\Users\\Prof DK\\Desktop\\TMMS2024")
```

# Set the seed for reproducibility
```{r}
set.seed(2024) ## This line sets the random seed for the analysis
```

* Random seeds are used to ensure reproducibility. 

* By setting the seed to 2024, you're telling the program to always start with the same "random" starting point when generating random numbers needed for the analysis. 

* This is helpful for debugging or comparing results across different runs.

## Installation and loading of necessary packages/libraries

# Loading libraries
```{r}
library(caret) #for training machine learning models
library(psych) ##for description of  data
library(ggplot2) ##for data visualization
library(caretEnsemble)##enables the creation of ensemble models
library(tidyverse) ##for data manipulation
library(mlbench)  ## For benchmarking ML Models
library(flextable) ## To create and style tables
library(mltools) #for hyperparameter tuning
library(tictoc) #for determining the time taken for a model to run
library(ROSE)  ## for random oversampling
library(smotefamily) ## for smote sampling
library(ROCR) ##For ROC curve
library(pROC) ## For visualizing, smoothing, and comparing ROC curves
library(e1071) ## For statistical modeling and  machine learning tasks
library(class) ## For classification using k-Nearest Neighbors and other methods
library(caTools) ## For splitting data into training and testing sets
library(MASS) ## Provides plotting functions and datasets
library(ISLR) ## for practical applications of statistical learning methods
library(boot) ## Useful for performing bootstrap resampling
library(cvTools) ## Contains functions for cross-validation, bootstrapping, and other resampling methods
```

# ----------------------------------------------------
## Resampling Techniques for Handling Data Imbalance
# ----------------------------------------------------
● Oversampling
● Undersampling
● Combined Resampling

 * Resampling techniques are a common set of strategies used to address data imbalance in machine learning. 
 
 * These techniques involve modifying the dataset by either increasing the number of minority class samples (oversampling) or 
 * reducing the number of majority class samples (undersampling). Here are some key resampling techniques:
 
# 1. Oversampling:

#● Random Oversampling: 
In this method, random instances from the minority class are duplicated until a more balanced distribution is achieved. While this can balance the class distribution, it may lead to overfitting.
  
#● SMOTE (Synthetic Minority Over-sampling Technique): 
SMOTE generates synthetic instances for the minority class by interpolating between neighboring instances. This approach creates new, realistic data points and helps prevent overfitting compared to random oversampling.

#● ADASYN (Adaptive Synthetic Sampling)
  * Description: An extension of SMOTE that focuses on generating more synthetic data for minority class examples that are harder to learn.
  * Advantages: Improves the focus on difficult minority class examples, potentially enhancing model performance.
  * Disadvantages: Similar to SMOTE, it can introduce noise if not applied carefully.

#● SMOTEN
#● SVM-SMOTE 
#● Random oversampler
#● Kmeans-SMOTE

# 2. Undersampling

#● Random Undersampling
  * Description: Involves randomly removing examples from the majority class to balance the dataset.
  * Advantages: Reduces the size of the dataset, making the training process faster.
  * Disadvantages: Can lead to loss of valuable information and underfitting.

#● Tomek Links
  *Description: Removes examples from the majority class that are close to minority class examples, forming Tomek links.
  *Advantages: Helps clean the boundary between classes, improving model performance.
  *Disadvantages: Only removes a small number of majority class examples, may not fully balance the dataset.
  
#● Random undersampler  
#● NearMiss
#● condensed Nearest Neighbour
#● Edited Nearest Neaghbour
  
# 3. Combined Resampling

#● Hybrid Methods
 *Description: Combines several resampling techniques to leverage their strengths and mitigate their weaknesses.
 *Advantages: Can provide a more balanced and effective approach.
 *Disadvantages: More complex to implement and require careful tuning.
 
## Consideration for Effective Resampling
* Understand Your Data: Know the extent and impact of imbalance.
* Evaluate Multiple Techniques: Different techniques might work better for different datasets.
* Cross-Validation: Use cross-validation to ensure that the model generalizes well.
* Performance Metrics: Focus on metrics like F1-score, precision, recall, and AUC-ROC instead of accuracy.

# ---------------------------------------------------- 
## Loading the given Malaria data
# ----------------------------------------------------
```{r}
mdata = read.csv("Malaria-Data.csv", header = TRUE)
```

```{r}
attach(mdata)
dim(mdata)
head(mdata,5)
names(mdata)
str(mdata)
summary(mdata)   ###Descriptive Statistics
describe(mdata)  ###Descriptive Statistics
sum(is.na(mdata)) ###Check for missing data
```

# Rename the classes of the Target variable and plot it to determine imbalance
```{r}
mdata$severe_maleria <- factor(mdata$severe_maleria, 
                           levels = c(0,1), 
                           labels = c("Negative", "Positive"))
```


# Plot Target Variable
```{r}
plot(factor(severe_maleria), 
     names= c("Negative", "Positive"), 
     col=c(2,3), 
     ylim=c(0, 250), ylab= "Respondent", xlab= "Malaria Diagnosis")
box()
```

# Alternatively use of ggplot 
```{r}
ggplot(mdata, aes(x = factor(severe_maleria),fill = severe_maleria)) + 
  geom_bar() + 
  labs(x = "Malaria Diagnosis", y = "Respondent") +
  theme_bw()
```
# -------------------------------------------------
## DATA PARTITION
# -------------------------------------------------
```{r}
set.seed(2024)
index=sample(2, nrow(mdata),replace =TRUE, prob=c(0.70,0.30))
train=mdata[index==1,]
test= mdata[index==2,]
#Get the dimensions of your train and test data
dim(train)
dim(test)
```
# Now Let's train some machine learning models using package caret

* The caret R package (short for Classification and regression Training) to carry out machine learning tasks in RStudio

* The caret package offers a range of tools and models for classification and regression machine learning problems(Kuhn et al. 2021)

* In fact, it offers over 239 different machine learning models from which to choose. 

* Don’t worry, we don’t expect you to use them all!

# VIEW THE MODELS IN CARET

```{r}
models= getModelInfo()
#names(models)
```

# ----------------------------------------------------------------------------------------------
# Handle Imbalanced: Oversampled data 
# ----------------------------------------------------------------------------------------------

```{r}
over <- ovun.sample(factor(severe_maleria)~., data = train, method = "over")$data
```

# ----------------------------------------------------------------------------------------------
## Building Machine Learning Models
# ----------------------------------------------------------------------------------------------

# prepare training scheme for cross-validation
```{r}
 control <- trainControl(method="repeatedcv", number=10, repeats=5)
```

# Train an SVM model 
```{r}
 set.seed(2024)
 tic()
 over.svmModel <- train(severe_maleria~., data=over, method="svmRadial", trControl=control)
 toc()
 over.svmModel
 over.svmpred=predict(over.svmModel,newdata = test)
 over.SVM.cM<- confusionMatrix(over.svmpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
 over.SVM.cM
 over.m1<- over.SVM.cM$byClass[c(1, 2, 5, 7, 11)]
 over.m1
 #plotting confusion matrix
 over.SVM.cM$table
 fourfoldplot(over.SVM.cM$table, col=rainbow(4), main="Oversampled SVM Confusion Matrix")
```

# Train an Random Forest model
```{r}
set.seed(2024)
tic()
over.RFModel <- train(severe_maleria~., data=over, method="rf", trControl=control)
toc()
over.RFModel
over.RFpred=predict(over.RFModel,newdata = test)
over.RF.cM<- confusionMatrix(over.RFpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m2<- over.RF.cM$byClass[c(1, 2, 5, 7, 11)]
over.m2
#plotting confusion matrix
over.RF.cM$table
fourfoldplot(over.RF.cM$table, col=rainbow(4), main="Oversampled RF Confusion Matrix")
```

# Train a Logisitic Regression model
```{r}
set.seed(2024)
tic()
over.lrModel <- train(severe_maleria~., data=over, method="glm", trControl=control)
toc()
over.lrModel
over.lrpred=predict(over.lrModel,newdata = test)
over.lr.cM<- confusionMatrix(over.lrpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m3<- over.lr.cM$byClass[c(1, 2, 5, 7, 11)]
over.m3
#plotting confusion matrix
over.lr.cM$table
fourfoldplot(over.lr.cM$table, col=rainbow(4), main="Oversampled LR Confusion Matrix")
```

# Train an k- Nearest Neigbour model
```{r}
set.seed(2024)
tic()
over.knnModel <- train(severe_maleria~., data=over, method="knn", trControl=control)
toc()
over.knnModel
over.knnpred=predict(over.knnModel,newdata = test)
over.knn.cM<- confusionMatrix(over.knnpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m4<- over.knn.cM$byClass[c(1, 2, 5, 7, 11)]
over.m4
#plotting confusion matrix
over.knn.cM$table
fourfoldplot(over.knn.cM$table, col=rainbow(4), main="Oversampled KNN Confusion Matrix")
```

# Train a Neural Net model
```{r}
set.seed(2024)
tic()
over.nnModel <- train(severe_maleria~., data=over, method="nnet", trControl=control)
toc()
over.nnModel
over.nnpred=predict(over.nnModel,newdata = test)
over.nn.cM<- confusionMatrix(over.nnpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m5<- over.nn.cM$byClass[c(1, 2, 5, 7, 11)]
over.m5
#plotting confusion matrix
over.nn.cM$table
fourfoldplot(over.nn.cM$table, col=rainbow(4), main="Oversampled NN Confusion Matrix")
```

# Train a Naive Bayes model
```{r}
set.seed(2024)
tic()
over.nbModel <- train(severe_maleria~., data=over, method="nb", trControl=control)
toc()
over.nbModel
over.nbpred=predict(over.nbModel,newdata = test)
over.nb.cM<- confusionMatrix(over.nbpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m6<- over.nb.cM$byClass[c(1, 2, 5, 7, 11)]
over.m6
#plotting confusion matrix
over.nb.cM$table
fourfoldplot(over.nb.cM$table, col=rainbow(4), main="Oversampled NB Confusion Matrix")
```

# Train a Linear Discriminant Analysis model
```{r}
set.seed(2024)
over.ldaModel <- train(severe_maleria~., data=over, method="lda", trControl=control)
over.ldaModel
over.ldapred=predict(over.ldaModel,newdata = test)
over.lda.cM<- confusionMatrix(over.ldapred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m7<- over.lda.cM$byClass[c(1, 2, 5, 7, 11)]
over.m7
##plotting confusion matrix
over.lda.cM$table
fourfoldplot(over.lda.cM$table, col=rainbow(4), main="Imbalanced LDA Confusion Matrix")
```

# Train a Decision Tree model
```{r}
set.seed(2024)
over.DTModel <- train(severe_maleria~., data=over, method="rpart", trControl=control)
over.DTModel
over.DTpred=predict(over.DTModel,newdata = test)
over.DT.cM<- confusionMatrix(over.DTpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m8<- over.DT.cM$byClass[c(1, 2, 5, 7, 11)]
over.m8
##plotting confusion matrix
over.DT.cM$table
fourfoldplot(over.DT.cM$table, col=rainbow(4), main="Imbalanced Decision Tree Confusion Matrix")
```

# Train a Bagging model
```{r}
set.seed(2024)
over.bagModel <- train(severe_maleria~., data=over, method="treebag", trControl=control)
over.bagModel
over.bagpred=predict(over.bagModel,newdata = test)
over.bag.cM<- confusionMatrix(over.bagpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m9<- over.bag.cM$byClass[c(1, 2, 5, 7, 11)]
over.m9
#plotting confusion matrix
over.bag.cM$table
fourfoldplot(over.bag.cM$table, col=rainbow(4), main="Oversampled Bagging Confusion Matrix")
```

# Train a Boosting model
```{r}
set.seed(2024)
tic()
over.boModel <- train(severe_maleria~., data=over, method="ada", trControl=control)
toc()
over.boModel
over.bopred=predict(over.boModel,newdata = test)
over.bo.cM<- confusionMatrix(over.bopred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
over.m10<- over.bo.cM$byClass[c(1, 2, 5, 7, 11)]
over.m10
#plotting confusion matrix
over.bo.cM$table
fourfoldplot(over.bo.cM$table, col=rainbow(4), main="Oversampled Boosting Confusion Matrix")
```

############################### measure #########################################

```{r}
measure.score <-round(data.frame(SVM= over.m1, 
                                 RF= over.m2, 
                                 LR = over.m3, 
                                 KNN=over.m4, 
                                 NN=over.m5, 
                                 NB=over.m6, 
                                 LDA=over.m7, 
                                 DT=over.m8, 
                                 Bagging = over.m9, 
                                 Boosting= over.m10), 4)
#table(measure.score)
flextable(measure.score)
```

# collect all resamples and compare
```{r}
results <- resamples(list(SVM=over.svmModel, 
                          RF=over.RFModel,
                          LR=over.lrModel,
                          KNN=over.knnModel,
                          nn=over.nnModel,
                          NB=over.nbModel,
                          LDA=ldaModel,
                          DT=over.DTModel,
                          Bagging=over.bagModel,
                          Boosting=over.boModel))
```

```{r}
library(dplyr)
## summarize the distributions of the results 
summary(results)
```

# Box-and-whisker plot of results
This type of chart is used to visualize the distribution of data. It shows the following information:
 * The center of the data (usually the median)
 * The spread of the data (represented by the box)
 * The presence of any outliers (data points that fall outside a certain range)
 
```{r}
bwplot(results)
```


```{r}
## dot plots of results
dotplot(results)
```

# ----------------------------------------------------------------------------------------------
# Handle Imbalanced: Undersampled data 
# ----------------------------------------------------------------------------------------------

```{r}
under <- ovun.sample(severe_maleria~., data = train, method = "under")$data
```
# ----------------------------------------------------------------------------------------------
## Building Machine Learning Models
# ----------------------------------------------------------------------------------------------

# prepare training scheme for cross-validation
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=5)
```

# Train a SVM model
```{r}
set.seed(2024)
tic()
under.svmModel <- train(severe_maleria~., data=under, method="svmRadial", trControl=control)
toc()
under.svmModel
under.svmpred=predict(under.svmModel,newdata = test)
under.SVM.cM<- confusionMatrix(under.svmpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.SVM.cM
under.m1<- under.SVM.cM$byClass[c(1, 2, 5, 7, 11)]
under.m1
#plotting confusion matrix
under.SVM.cM$table
fourfoldplot(under.SVM.cM$table, col=rainbow(4), main="Undersampled SVM Confusion Matrix")
```

#Train a Random Forest model
```{r}
set.seed(2024)
tic()
under.RFModel <- train(severe_maleria~., data=under, method="rf", trControl=control)
toc()
under.RFModel
under.RFpred=predict(under.RFModel,newdata = test)
under.RF.cM<- confusionMatrix(under.RFpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m2<- under.RF.cM$byClass[c(1, 2, 5, 7, 11)]
under.m2
#plotting confusion matrix
under.RF.cM$table
fourfoldplot(under.RF.cM$table, col=rainbow(4), main="Undersampled RF Confusion Matrix")
```
# Train a Logisitic Regression model
```{r}
set.seed(2024)
tic()
under.lrModel <- train(severe_maleria~., data=under, method="glm", trControl=control)
toc()
under.lrModel
under.lrpred=predict(under.lrModel,newdata = test)
under.lr.cM<- confusionMatrix(under.lrpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m3<- under.lr.cM$byClass[c(1, 2, 5, 7, 11)]
under.m3
#plotting confusion matrix
under.lr.cM$table
fourfoldplot(under.lr.cM$table, col=rainbow(4), main="Undersampled LR Confusion Matrix")
```

# Train a k- Nearest Neigbour model

```{r}
set.seed(2024)
under.knnModel <- train(severe_maleria~., data=under, method="knn", trControl=control)
under.knnModel
under.knnpred=predict(under.knnModel,newdata = test)
under.knn.cM<- confusionMatrix(under.knnpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m4<- under.knn.cM$byClass[c(1, 2, 5, 7, 11)]
under.m4
#plotting confusion matrix
under.knn.cM$table
fourfoldplot(under.knn.cM$table, col=rainbow(4), main="Undersampled KNN Confusion Matrix")
```

# Train a Neural Net model
```{r}
set.seed(2024)
tic()
under.nnModel <- train(severe_maleria~., data=under, method="nnet", trControl=control)
toc()
under.nnModel
under.nnpred=predict(under.nnModel,newdata = test)
under.nn.cM<- confusionMatrix(under.nnpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m5<- under.nn.cM$byClass[c(1, 2, 5, 7, 11)]
under.m5
#plotting confusion matrix
under.nn.cM$table
fourfoldplot(under.nn.cM$table, col=rainbow(4), main="Undersampled NN Confusion Matrix")
```

# Train a Naive Bayes model
```{r}
set.seed(2024)
under.nbModel <- train(severe_maleria~., data=under, method="nb", trControl=control)
under.nbModel
under.nbpred=predict(under.nbModel,newdata = test)
under.nb.cM<- confusionMatrix(under.nbpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m6<- under.nb.cM$byClass[c(1, 2, 5, 7, 11)]
under.m6
#plotting confusion matrix
under.nb.cM$table
fourfoldplot(under.nb.cM$table, col=rainbow(4), main="Undersampled NB Confusion Matrix")
```

## Train a Linear Discriminant Analysis model
```{r}
set.seed(2024)
under.ldaModel <- train(severe_maleria~., data=under, method="lda", trControl=control)
under.ldaModel
under.ldapred=predict(under.ldaModel,newdata = test)
under.lda.cM<- confusionMatrix(under.ldapred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m7<- under.lda.cM$byClass[c(1, 2, 5, 7, 11)]
under.m7
##plotting confusion matrix
under.lda.cM$table
fourfoldplot(under.lda.cM$table, col=rainbow(4), main="Imbalanced LDA Confusion Matrix")
```

# Train a Decision Tree model
```{r}
set.seed(2024)
under.DTModel <- train(severe_maleria~., data=under, method="rpart", trControl=control)
under.DTModel
under.DTpred=predict(under.DTModel,newdata = test)
under.DT.cM<- confusionMatrix(under.DTpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m8<- under.DT.cM$byClass[c(1, 2, 5, 7, 11)]
under.m8
##plotting confusion matrix
under.DT.cM$table
fourfoldplot(under.DT.cM$table, col=rainbow(4), main="Imbalanced Decision Tree Confusion Matrix")
```
# Train a Bagging model
```{r}
set.seed(2024)
under.bagModel <- train(severe_maleria~., data=under, method="treebag", trControl=control)
under.bagModel
under.bagpred=predict(under.bagModel,newdata = test)
under.bag.cM<- confusionMatrix(under.bagpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m9<- under.bag.cM$byClass[c(1, 2, 5, 7, 11)]
under.m9
#plotting confusion matrix
under.bag.cM$table
fourfoldplot(under.bag.cM$table, col=rainbow(4), main="Undersampled Bagging Confusion Matrix")
```

# Train a Boosting model
```{r}
set.seed(2024)
tic()
under.boModel <- train(severe_maleria~., data=under, method="ada", trControl=control)
toc()
under.boModel
under.bopred=predict(under.boModel,newdata = test)
under.bo.cM<- confusionMatrix(under.bopred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
under.m10<- under.bo.cM$byClass[c(1, 2, 5, 7, 11)]
under.m10
#plotting confusion matrix
under.bo.cM$table
fourfoldplot(under.bo.cM$table, col=rainbow(4), main="Undersampled Boosting Confusion Matrix")
```

# ------------------------------------------------------TABULATE THE MEASURES -------------------------------------------------

```{r}
measure.score <-round(data.frame(SVM= under.m1, 
                                 RF= under.m2, 
                                 LR = under.m3, 
                                 KNN=under.m4, 
                                 NN=under.m5, 
                                 NB=under.m6, 
                                 LDA=under.m7, 
                                 DT=under.m8, 
                                 Bagging = under.m9, 
                                 Boosting = under.m10), 4)
#table(measure.score)
flextable(measure.score)
```

# Collect all resamples and compare
```{r}
results <- resamples(list(SVM=under.svmModel, 
                          RF=under.RFModel,
                          LR=under.lrModel,
                          KNN=under.knnModel, 
                          NN=under.nnModel, 
                          NB=under.nbModel,
                          LDA=ldaModel,
                          Bagging=under.bagModel,
                          boosting=under.boModel))
```

# Summarize the distribution of the results
```{r}
summary(results)
```

# Box-and-whisker plot of results
This type of chart is used to visualize the distribution of data. It shows the following information:
 * The center of the data (usually the median)
 * The spread of the data (represented by the box)
 * The presence of any outliers (data points that fall outside a certain range)

```{r}
bwplot(results, main ="Comparison of models")
```

# Dot plots of results
```{r}
dotplot(results)
```

# ----------------------------------------------------------------------------------------------
# Handle Imbalanced: Hybrid Method 
# ----------------------------------------------------------------------------------------------
```{r}
library(dplyr)
hybrid <- ovun.sample(severe_maleria~., data = train, method = "both")$data
```

# ----------------------------------------------------------------------------------------------
## Building Machine Learning Models
# ----------------------------------------------------------------------------------------------

# prepare training scheme for cross-validation
```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=5)
```

# Train a SVM model
```{r}
set.seed(2024)
#both.svmModel <- train(severe_maleria~., data=hybrid, method="svmRadial", trControl=control)
#both.svmModel
both.svmpred=predict(both.svmModel,newdata = test)
both.SVM.cM<- confusionMatrix(both.svmpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.SVM.cM
both.m1<- both.SVM.cM$byClass[c(1, 2, 5, 7, 11)]
both.m1
#plotting confusion matrix
both.SVM.cM$table
fourfoldplot(both.SVM.cM$table, col=rainbow(4), main="Hybrid SVM Confusion Matrix")
```

```{r}
set.seed(2024)
tic()
both.svmModel <- train(severe_maleria~., data=hybrid, method="svmRadial", trControl=control, tuneLength=10)
toc()
both.svmModel
plot(both.svmModel)
both.svmpred=predict(both.svmModel,newdata = test)
both.SVM.cM<- confusionMatrix(both.svmpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.SVM.cM
both.m1<- both.SVM.cM$byClass[c(1, 2, 5, 7, 11)]
both.m1
#plotting confusion matrix
both.SVM.cM$table
fourfoldplot(both.SVM.cM$table, col=rainbow(4), main="Hybrid SVM Confusion Matrix")
```
# Train a Random Forest model
```{r}
set.seed(2024)
tic()
both.RFModel <- train(severe_maleria~., data=hybrid, method="rf", trControl=control)
toc()
both.RFModel
both.RFpred=predict(both.RFModel,newdata = test)
both.RF.cM<- confusionMatrix(both.RFpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m2<- both.RF.cM$byClass[c(1, 2, 5, 7, 11)]
both.m2
#plotting confusion matrix
both.RF.cM$table
fourfoldplot(both.RF.cM$table, col=rainbow(4), main="Hybrid RF Confusion Matrix")
```

# Train a Logisitic Regression model
```{r}
set.seed(2024)
both.lrModel <- train(severe_maleria~., data=hybrid, method="glm", trControl=control)
both.lrModel
both.lrpred=predict(both.lrModel,newdata = test)
both.lr.cM<- confusionMatrix(both.lrpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m3<- both.lr.cM$byClass[c(1, 2, 5, 7, 11)]
both.m3
#plotting confusion matrix
both.lr.cM$table
fourfoldplot(both.lr.cM$table, col=rainbow(4), main="Hybrid LR Confusion Matrix")
```
# Train a k- Nearest Neigbour model
```{r}
set.seed(2024)
both.knnModel <- train(severe_maleria~., data=hybrid, method="knn", trControl=control)
both.knnModel
both.knnpred=predict(both.knnModel,newdata = test)
both.knn.cM<- confusionMatrix(both.knnpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m4<- both.knn.cM$byClass[c(1, 2, 5, 7, 11)]
both.m4
#plotting confusion matrix
both.knn.cM$table
fourfoldplot(both.knn.cM$table, col=rainbow(4), main="Hybrid KNN Confusion Matrix")
```
# Train a Neural Net model
```{r}
set.seed(2024)
tic()
both.nnModel <- train(severe_maleria~., data=hybrid, method="nnet", trControl=control)
toc()
both.nnModel
both.nnpred=predict(both.nnModel,newdata = test)
both.nn.cM<- confusionMatrix(both.nnpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m5<- both.nn.cM$byClass[c(1, 2, 5, 7, 11)]
both.m5
#plotting confusion matrix
both.nn.cM$table
fourfoldplot(both.nn.cM$table, col=rainbow(4), main="Hybrid NN Confusion Matrix")
```
# Train a Naive Bayes model
```{r}
set.seed(2024)
both.nbModel <- train(severe_maleria~., data=hybrid, method="nb", trControl=control)
both.nbModel
both.nbpred=predict(both.nbModel,newdata = test)
both.nb.cM<- confusionMatrix(both.nbpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m6<- both.nb.cM$byClass[c(1, 2, 5, 7, 11)]
both.m6
#plotting confusion matrix
both.nb.cM$table
fourfoldplot(both.nb.cM$table, col=rainbow(4), main="Hybrid NB Confusion Matrix")
```
# train a Linear Discriminant Analysis model
```{r}
set.seed(2024)
both.ldaModel <- train(severe_maleria~., data=hybrid, method="lda", trControl=control)
both.ldaModel
both.ldapred=predict(both.ldaModel,newdata = test)
both.lda.cM<- confusionMatrix(both.ldapred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m7<- both.lda.cM$byClass[c(1, 2, 5, 7, 11)]
both.m7
##plotting confusion matrix
both.lda.cM$table
fourfoldplot(both.lda.cM$table, col=rainbow(4), main="Imbalanced LDA Confusion Matrix")
```

# Train a Decision Tree model
```{r}
set.seed(2024)
both.DTModel <- train(severe_maleria~., data=hybrid, method="rpart", trControl=control)
both.DTModel
both.DTpred=predict(both.DTModel,newdata = test)
both.DT.cM<- confusionMatrix(both.DTpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m8<- both.DT.cM$byClass[c(1, 2, 5, 7, 11)]
both.m8
##plotting confusion matrix
both.DT.cM$table
fourfoldplot(both.DT.cM$table, col=rainbow(4), main="Imbalanced LDA Confusion Matrix")
```


# Train a Bagging model
```{r}
set.seed(2024)
both.bagModel <- train(severe_maleria~., data=hybrid, method="treebag", trControl=control)
both.bagModel
both.bagpred=predict(both.bagModel,newdata = test)
both.bag.cM<- confusionMatrix(both.bagpred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m9<- both.bag.cM$byClass[c(1, 2, 5, 7, 11)]
both.m9
#plotting confusion matrix
both.bag.cM$table
fourfoldplot(both.bag.cM$table, col=rainbow(4), main="Hybrid Bagging Confusion Matrix")
```
# Train a Boosting model
```{r}
set.seed(2024)
tic()
both.boModel <- train(severe_maleria~., data=hybrid, method="ada", trControl=control)
toc()
both.boModel
both.bopred=predict(both.boModel,newdata = test)
both.bo.cM<- confusionMatrix(both.bopred,as.factor(test$severe_maleria), positive = 'Negative', mode='everything')
both.m10<- both.bo.cM$byClass[c(1, 2, 5, 7, 11)]
both.m10
#plotting confusion matrix
both.bo.cM$table
fourfoldplot(both.bo.cM$table, col=rainbow(4), main="Hybrid Boosting Confusion Matrix")
```
############################### measure #########################################
```{r}
measure.score <-round(data.frame(SVM=both.m1, 
                                 RF=both.m2, 
                                 LR=both.m3, 
                                 KNN=both.m4, 
                                 NN=both.m5, 
                                 NB=both.m6, 
                                 LDA=both.m7, 
                                 DT=both.m8,
                                 Bagging=both.m9, 
                                 Boosting=both.m10), 4)
flextable(measure.score)
```


```{r}
results <- resamples(list(SVM=both.svmModel, 
                          RF=both.RFModel,
                          LR=both.lrModel,
                          KNN=both.knnModel,
                          nn=both.nnModel,
                          NB=both.nbModel,
                          LDA=both.ldaModel,
                          DT=both.DTModel,
                          Bagging=both.bagModel,
                          Boosting=both.boModel))
```

# summarize the distributions of the results 
```{r}
library(dplyr)
summary(results)
```

# Box-and-whisker plot of results
This type of chart is used to visualize the distribution of data. It shows the following information:
 * The center of the data (usually the median)
 * The spread of the data (represented by the box)
 * The presence of any outliers (data points that fall outside a certain range)
 
```{r}
## boxplots of results
bwplot(results)
```

```{r}
## dot plots of results
dotplot(results)
```

```{r}

```
















